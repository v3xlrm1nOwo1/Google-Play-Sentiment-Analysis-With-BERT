{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v3xlrm1nOwo1/Google-Play-Sentiment-Analysis-With-BERT/blob/main/02_Text_Preprocessing_and_Create_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X126oZFhtrfH"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sNzkL1fjtrbc"
      },
      "outputs": [],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHFSs8_cZLdk"
      },
      "outputs": [],
      "source": [
        "!pip install -qq transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H1eTzH6TZOTz"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textwrap import wrap\n",
        "from collections import defaultdict\n",
        "from accelerate import Accelerator\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN-6qRvYKJfU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 6, 4\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "LEARN_RATE = 3e-6\n",
        "RANDOM_SEED = 666\n",
        "MAX_LEN = 160\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "CHECKPOINT= 'bert-base-cased'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rViZv3CB3e-"
      },
      "source": [
        "## The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTbbGw1Uo3eW"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('reviews.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvmTBp_gpb5j"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS6NZNCUpwYG"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSadgdRfpyYD"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=df.score)\n",
        "plt.xlabel('review score');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn-dzbv9B84-"
      },
      "source": [
        "## Add More Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXsCrCKmqi7x"
      },
      "outputs": [],
      "source": [
        "def to_sentiment(rating):\n",
        "    rating = int(rating)\n",
        "    if rating <= 2:\n",
        "        return 0\n",
        "    elif rating == 3:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D32WL3APq5dw"
      },
      "outputs": [],
      "source": [
        "df['sentiment'] = df.score.apply(to_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mc6lfUCrUxg"
      },
      "outputs": [],
      "source": [
        "df.head(n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOQ8y753udwd"
      },
      "outputs": [],
      "source": [
        "class_names = ['negative', 'neutral', 'postive']\n",
        "\n",
        "ax = sns.countplot(x=df.sentiment)\n",
        "plt.xlabel('review sentiment')\n",
        "ax.set_xticklabels(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLHx5WgHBmEG"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p1fRNNCvAhO"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(CHECKPOINT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vtiEA2Fxenl"
      },
      "outputs": [],
      "source": [
        "text = 'Sharing pretrained models!'\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH0tNWGJ1ba8"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "print(len(tokens))\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AR_UahD17tq"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens=tokens)\n",
        "\n",
        "print(len(token_ids))\n",
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-BcGae02Utq"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.tokenize(text)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f' Sentence: {text}')\n",
        "print(f'   Tokens: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCQqUCZF3I9t"
      },
      "outputs": [],
      "source": [
        "tokenizer.sep_token, tokenizer.sep_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7rlRTXD3SFg"
      },
      "outputs": [],
      "source": [
        "tokenizer.cls_token, tokenizer.cls_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5DmObNP3VhU"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token, tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8Xz1I8Q3YMh"
      },
      "outputs": [],
      "source": [
        "tokenizer.unk_token, tokenizer.unk_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Cs9MATr3bak"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  text=text,\n",
        "  max_length=10,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=True,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39O_gFXl3tgB"
      },
      "outputs": [],
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QJrjc5C3-RG"
      },
      "outputs": [],
      "source": [
        "print(len(encoding['attention_mask'][0]))\n",
        "encoding['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_6jEmYy4Hrr"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujHgBb304NjE"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in df.content:\n",
        "  tokens = tokenizer.encode(txt, max_length=512)\n",
        "  token_lens.append(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4NkBdIr5CA5"
      },
      "outputs": [],
      "source": [
        "sns.distplot(token_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Token count');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbboQPkXLBrk"
      },
      "source": [
        "## Create a Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC8mp4j2mHz5"
      },
      "outputs": [],
      "source": [
        "dataset_df = df[['content', 'sentiment']]\n",
        "\n",
        "dataset_df.head(n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ0i1eOd5Ghn"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(dataset_df, test_size=0.2, random_state=RANDOM_SEED)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I99qzB5osW87"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ClW0TVtB_wm"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len, include_raw_text=False):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.include_raw_text = include_raw_text\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      truncation=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    output =  {\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "    if self.include_raw_text:\n",
        "        output['review_text'] = review\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvJq0IaoBkrw"
      },
      "outputs": [],
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29oS8thKS6GM"
      },
      "outputs": [],
      "source": [
        "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVoofUSWCrMg"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE, include_raw_text=False):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.content.to_numpy(),\n",
        "    targets=df.sentiment.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len,\n",
        "    include_raw_text=include_raw_text\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4,\n",
        "    # collate_fn=data_collator\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SxzI4gSDIkv"
      },
      "outputs": [],
      "source": [
        "train_data_loader = create_data_loader(df=df_train, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df=df_val, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE, include_raw_text=True)\n",
        "test_data_loader = create_data_loader(df=df_test, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE, include_raw_text=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZC6ViQbEBGX"
      },
      "outputs": [],
      "source": [
        "data = next(iter(val_data_loader))\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SNOLxOLEHb_"
      },
      "outputs": [],
      "source": [
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMIHb5PZWxuk"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes=3):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.model = transformers.BertModel.from_pretrained(CHECKPOINT)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.model.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        pooled_output = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )[1]\n",
        "\n",
        "        output = self.drop(pooled_output)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hk7FYUbjc1K5"
      },
      "outputs": [],
      "source": [
        "model = SentimentClassifier(n_classes=3)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6pskzH0u9ItP"
      },
      "outputs": [],
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "\n",
        "print(input_ids.shape) # batch size x seq length\n",
        "print(attention_mask.shape) # batch size x seq length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9paMSCsa9Iqj"
      },
      "outputs": [],
      "source": [
        "# F.softmax(model(input_ids, attention_mask), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FlSUAZ28ffkU"
      },
      "outputs": [],
      "source": [
        "optimizer = transformers.AdamW(model.parameters(), lr=LEARN_RATE, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dAjX6XpnsuX"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, scheduler, n_examples):\n",
        "    model = model.train()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for dl in data_loader:\n",
        "        input_ids = dl['input_ids'].to(device)\n",
        "        attention_mask = dl['attention_mask'].to(device)\n",
        "        targets = dl['targets'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        losses.append(loss)\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return  correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkVZ6jOCr6AL"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d['input_ids'].to(device)\n",
        "      attention_mask = d['attention_mask'].to(device)\n",
        "      targets = d['targets'].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss)\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH40TuXjsUrl"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print('=' * 50)\n",
        "  print(f'Epoch {epoch + 1} / {EPOCHS}')\n",
        "\n",
        "  train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, scheduler, len(df_train))\n",
        "  print(f'Train loss {train_loss}, accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, device, len(df_val))\n",
        "  print(f'Val loss {val_loss}, accuracy {val_acc}')\n",
        "\n",
        "  print(f'===> Epoch {epoch + 1} / {EPOCHS} | Train loss {train_loss}, accuracy {train_acc} | Val loss {val_loss}, accuracy {val_acc}')\n",
        "  print('=' * 50)\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeM9PRN4tXrv"
      },
      "outputs": [],
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRNpTpHJaDRkjSnPjZEtim",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}